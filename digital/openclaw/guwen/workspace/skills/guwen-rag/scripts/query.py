#!/usr/bin/env python3
"""
Single query mode - retrieve and generate response using sqlite-vec
"""
import os
import sys
import struct
import json
import requests

# Add parent to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from references.config import get_db, OLLAMA_HOST, EMBEDDING_MODEL, CHAT_MODEL, TOP_K_RESULTS

def get_query_embedding(query):
    """Get embedding for query"""
    response = requests.post(
        f"{OLLAMA_HOST}/api/embeddings",
        json={"model": EMBEDDING_MODEL, "prompt": query}
    )
    if response.status_code == 200:
        return response.json()["embedding"]
    else:
        raise Exception(f"Embedding failed: {response.text}")

def search_collection(query_embedding, top_k=TOP_K_RESULTS):
    """Search sqlite-vec for relevant documents"""
    query_bytes = struct.pack(f'{len(query_embedding)}f', *query_embedding)
    
    db = get_db()
    # Use k = ? constraint instead of LIMIT
    results = db.execute("""
        SELECT text, source, distance 
        FROM documents 
        WHERE embedding MATCH ?
          AND k = ?
        ORDER BY distance
    """, (query_bytes, top_k)).fetchall()
    
    return results

def format_context(results):
    """Format retrieved documents into context string"""
    context_parts = []
    
    for i, (text, source, dist) in enumerate(results):
        meta = json.loads(source) if source else {}
        filename = meta.get("filename", "Unknown")
        title = meta.get("title", "")
        context_parts.append(f"\n[æ¥æº {i+1}: {filename} - {title} (dist: {dist:.3f})]\n{text}\n")
    
    return "\n".join(context_parts)

def generate_response(query, context):
    """Generate response using Ollama"""
    
    system_prompt = f"""ä½ æ˜¯ç»å­¦å¤§å¸ˆï¼ˆClassical Chinese Scholar & Philosopherï¼‰ã€‚

ä½ çš„çŸ¥è¯†åº“æ¥æºäºä»¥ä¸‹ç»å…¸æ–‡æœ¬ã€‚å›ç­”æ—¶åŠ¡å¿…éµå¾ªã€åŸæ–‡ã€‘ã€ä»Šè¯‘ã€‘ã€å¾®æ—¨ã€‘ä¸‰å å¼æ ¼å¼ã€‚

ã€åŸæ–‡ã€‘ï¼šå…ˆå¼•ç”¨ç›¸å…³åŸæ–‡
ã€ä»Šè¯‘ã€‘ï¼šæä¾›ç²¾å‡†ç™½è¯æ–‡ç¿»è¯‘  
ã€å¾®æ—¨ã€‘ï¼šè§£æå“²å­¦å†…æ¶µï¼Œç‰¹åˆ«æ³¨æ„è™šè¯ï¼ˆä¹‹ã€ä¹ã€è€…ã€ä¹Ÿã€çŸ£ã€ç„‰ï¼‰çš„ç”¨æ³•

è¯­æ°”éœ€å„’é›…ã€è°¦é€Šï¼Œç§°ç”¨æˆ·ä¸º"å­¦å‹"ã€‚

ç›¸å…³æ–‡æœ¬ï¼š
{context}
"""
    
    prompt = f"å­¦å‹é—®ï¼š{query}\n\nè¯·ä¾æ®åœ£å…¸ä½œç­”ï¼š"
    
    response = requests.post(
        f"{OLLAMA_HOST}/api/generate",
        json={
            "model": CHAT_MODEL,
            "system": system_prompt,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.7,
                "num_predict": 2048
            }
        }
    )
    
    if response.status_code == 200:
        return response.json()["response"]
    else:
        raise Exception(f"Generation failed: {response.text}")

def query(query_text):
    """Main query function"""
    print(f"ğŸ” å­¦å‹é—®ï¼š{query_text}\n")
    
    # Get embedding
    print("  ç”ŸæˆæŸ¥è¯¢å‘é‡...")
    query_embedding = get_query_embedding(query_text)
    
    # Search
    print("  æ£€ç´¢ç›¸å…³æ–‡æœ¬...")
    results = search_collection(query_embedding, TOP_K_RESULTS)
    
    # Format context
    context = format_context(results)
    print(f"  æ‰¾åˆ° {len(results)} ç¯‡ç›¸å…³æ–‡æœ¬\n")
    
    # Generate
    print("  ç”Ÿæˆå›ç­”...\n")
    print("=" * 60)
    response = generate_response(query_text, context)
    print(response)
    print("=" * 60)
    
    return response

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Query the RAG system")
    parser.add_argument("query", help="Query text")
    
    args = parser.parse_args()
    query(args.query)
