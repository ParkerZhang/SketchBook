#!/usr/bin/env python3
"""
Conversational chat mode with context memory using sqlite-vec
"""
import os
import sys
import struct
import json
import requests

# Add parent to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from references.config import get_db, OLLAMA_HOST, EMBEDDING_MODEL, CHAT_MODEL, TOP_K_RESULTS

class ConversationMemory:
    def __init__(self, max_history=5):
        self.history = []
        self.max_history = max_history
    
    def add(self, role, content):
        self.history.append({"role": role, "content": content})
        if len(self.history) > self.max_history * 2:
            self.history = self.history[-self.max_history * 2:]
    
    def get_context(self):
        return "\n".join([f"{h['role']}: {h['content']}" for h in self.history])

def get_embedding(text):
    response = requests.post(
        f"{OLLAMA_HOST}/api/embeddings",
        json={"model": EMBEDDING_MODEL, "prompt": text}
    )
    return response.json()["embedding"]

def search_documents(query_embedding):
    """Search sqlite-vec for relevant documents"""
    query_bytes = struct.pack(f'{len(query_embedding)}f', *query_embedding)
    
    db = get_db()
    results = db.execute("""
        SELECT text, source, distance 
        FROM documents 
        WHERE embedding MATCH ?
          AND k = ?
        ORDER BY distance
    """, (query_bytes, TOP_K_RESULTS)).fetchall()
    
    # Format results
    docs = []
    for text, source, dist in results:
        meta = json.loads(source) if source else {}
        filename = meta.get("filename", "Unknown")
        docs.append(f"[{filename}]\n{text}")
    
    return "\n\n---\n\n".join(docs)

def chat_response(query, conversation_history, retrieved_context):
    system_prompt = f"""ä½ æ˜¯ç»å­¦å¤§å¸ˆï¼ˆClassical Chinese Scholar & Philosopherï¼‰ã€‚

å›ç­”æ—¶éµå¾ªã€åŸæ–‡ã€‘ã€ä»Šè¯‘ã€‘ã€å¾®æ—¨ã€‘ä¸‰å å¼ï¼š
- ã€åŸæ–‡ã€‘ï¼šå…ˆå¼•ç›¸å…³åŸæ–‡
- ã€ä»Šè¯‘ã€‘ï¼šç™½è¯ç¿»è¯‘  
- ã€å¾®æ—¨ã€‘ï¼šè§£æå“²å­¦ï¼Œæ³¨æ„è™šè¯

è¯­æ°”å„’é›…è°¦é€Šï¼Œç§°ç”¨æˆ·ä¸º"å­¦å‹"ã€‚

ç›¸å…³æ–‡æœ¬ï¼š
{retrieved_context}

å¯¹è¯å†å²ï¼š
{conversation_history}
"""
    
    response = requests.post(
        f"{OLLAMA_HOST}/api/generate",
        json={
            "model": CHAT_MODEL,
            "system": system_prompt,
            "prompt": f"å­¦å‹é—®ï¼š{query}",
            "stream": False,
            "options": {"temperature": 0.7, "num_predict": 2048}
        }
    )
    
    return response.json()["response"]

def interactive_chat():
    memory = ConversationMemory(max_history=5)
    
    print("=" * 60)
    print("ğŸ›ï¸  ç»å­¦å¤§å¸ˆ RAG å¯¹è¯ç³»ç»Ÿ")
    print("è¾“å…¥é—®é¢˜ä¸è´«é“ç ”è®¨å…¸ç±ï¼Œè¾“å…¥ 'exit' é€€å‡º")
    print("=" * 60 + "\n")
    
    while True:
        user_input = input("å­¦å‹ï¼š").strip()
        
        if user_input.lower() in ['exit', 'quit', 'é€€å‡º']:
            print("\nå­¦å‹çé‡ï¼Œå†ä¼šã€‚ğŸ“œ")
            break
        
        if not user_input:
            continue
        
        # Retrieve relevant context
        print("  æ£€ç´¢ä¸­...")
        query_embedding = get_embedding(user_input)
        context = search_documents(query_embedding)
        
        # Generate response
        conv_history = memory.get_context()
        response = chat_response(user_input, conv_history, context)
        
        # Display
        print(f"\nğŸ“– {response}\n")
        
        # Update memory
        memory.add("å­¦å‹", user_input)
        memory.add("å¤§å¸ˆ", response)

if __name__ == "__main__":
    interactive_chat()

